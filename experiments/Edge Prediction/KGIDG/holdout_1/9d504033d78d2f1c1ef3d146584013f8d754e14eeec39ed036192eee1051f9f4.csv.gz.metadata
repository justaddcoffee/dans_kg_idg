{
    "creation_time": 1699163433.4940832,
    "creation_time_human": "2023-11-05 01:50:33",
    "time_delta": 2996.1292169094086,
    "time_delta_human": "49 minutes and 56 seconds",
    "file_dump_time": 0.0028929710388183594,
    "file_dump_time_human": "0 seconds",
    "file_dump_size": 1792,
    "file_dump_size_human": "1.8 kB",
    "load_kwargs": {},
    "dump_kwargs": {},
    "function_name": "_evaluate_on_single_holdout",
    "function_file": "/Users/jtr4v/PythonProject/dans_kg_idg/venv/lib/python3.11/site-packages/embiggen/utils/abstract_models/abstract_classifier_model.py:2355",
    "args_to_ignore": [
        "verbose",
        "smoke_test",
        "number_of_holdouts",
        "metadata"
    ],
    "source": "    @classmethod\n    @Cache(\n        cache_path=\"{cache_dir}/{cls.task_name()}/{graph.get_name()}/holdout_{holdout_number}/{_hash}.csv.gz\",\n        cache_dir=\"experiments\",\n        enable_cache_arg_name=\"enable_cache\",\n        args_to_ignore=[\"verbose\", \"smoke_test\", \"number_of_holdouts\", \"metadata\"],\n        capture_enable_cache_arg_name=False,\n        use_approximated_hash=True,\n    )\n    def _evaluate_on_single_holdout(\n        cls,\n        models: Union[\n            Type[\"AbstractClassifierModel\"], List[Type[\"AbstractClassifierModel\"]]\n        ],\n        library_names: Optional[Union[str, List[str]]],\n        graph: Graph,\n        subgraph_of_interest: Graph,\n        use_subgraph_as_support: bool,\n        node_features: Optional[List[np.ndarray]],\n        node_type_features: Optional[List[np.ndarray]],\n        edge_type_features: Optional[List[np.ndarray]],\n        edge_features: Optional[List[np.ndarray]],\n        node_features_preprocessing_steps: Optional[\n            Union[\n                Type[AbstractFeaturePreprocessor],\n                List[Type[AbstractFeaturePreprocessor]],\n            ]\n        ],\n        random_state: int,\n        holdout_number: int,\n        number_of_holdouts: int,\n        evaluation_schema: str,\n        enable_cache: bool,\n        smoke_test: bool,\n        holdouts_kwargs: Dict[str, Any],\n        subgraph_of_interest_has_compatible_nodes: Optional[bool],\n        features_names: List[str],\n        features_parameters: Dict[str, Any],\n        metadata: Dict[str, Any],\n        verbose: bool,\n        **validation_kwargs,\n    ) -> pd.DataFrame:\n        starting_setting_up_holdout = time.time()\n\n        # We create the graph split using the provided schema.\n        train, test = cls.split_graph_following_evaluation_schema(\n            graph=graph,\n            evaluation_schema=evaluation_schema,\n            random_state=random_state,\n            holdout_number=holdout_number,\n            number_of_holdouts=number_of_holdouts,\n            **holdouts_kwargs,\n        )\n\n        # We compute the remaining features\n        starting_to_compute_node_features = time.time()\n        holdout_node_features = cls.normalize_node_features(\n            graph=train,\n            support=train,\n            random_state=random_state * (holdout_number + 1),\n            node_features=node_features,\n            node_features_preprocessing_steps=node_features_preprocessing_steps,\n            allow_automatic_feature=True,\n            skip_evaluation_biased_feature=False,\n            smoke_test=smoke_test,\n            precompute_constant_stocastic_features=True,\n        )\n        time_required_to_compute_node_features = (\n            time.time() - starting_to_compute_node_features\n        )\n\n        # We compute the remaining features\n        starting_to_compute_node_type_features = time.time()\n        holdout_node_type_features = cls.normalize_node_type_features(\n            graph=train,\n            support=train,\n            random_state=random_state * (holdout_number + 1),\n            node_type_features=node_type_features,\n            allow_automatic_feature=True,\n            skip_evaluation_biased_feature=False,\n            smoke_test=smoke_test,\n            precompute_constant_stocastic_features=True,\n        )\n        time_required_to_compute_node_type_features = (\n            time.time() - starting_to_compute_node_type_features\n        )\n\n        # We compute the remaining features\n        starting_to_compute_edge_type_features = time.time()\n        holdout_edge_type_features = cls.normalize_edge_type_features(\n            graph=train,\n            support=train,\n            random_state=random_state * (holdout_number + 1),\n            edge_type_features=edge_type_features,\n            allow_automatic_feature=True,\n            skip_evaluation_biased_feature=False,\n            smoke_test=smoke_test,\n            precompute_constant_stocastic_features=True,\n        )\n        time_required_to_compute_edge_type_features = (\n            time.time() - starting_to_compute_edge_type_features\n        )\n\n        # We execute the same thing as described above,\n        # but now for the edge features instead that for\n        # the node features.\n        starting_to_compute_edge_features = time.time()\n        holdout_edge_features = cls.normalize_edge_features(\n            graph=train,\n            support=train,\n            random_state=random_state * (holdout_number + 1),\n            edge_features=edge_features,\n            allow_automatic_feature=True,\n            skip_evaluation_biased_feature=False,\n            smoke_test=smoke_test,\n            precompute_constant_stocastic_features=True,\n        )\n        time_required_to_compute_edge_features = (\n            time.time() - starting_to_compute_edge_features\n        )\n\n        if subgraph_of_interest is not None:\n            # First we align the train and test graph to have\n            # the same node dictionary of the subgraph of interest\n            # when the subgraph of interest does not have\n            # the same node dictionary as the original graph.\n            if not subgraph_of_interest_has_compatible_nodes:\n                train = train.filter_from_names(\n                    node_names_to_keep_from_graph=subgraph_of_interest\n                )\n                test = test.filter_from_names(\n                    node_names_to_keep_from_graph=subgraph_of_interest\n                )\n\n                # We adjust the node features to only include the node features\n                # that the subgraph of interest allows us to use.\n                if holdout_node_features is not None:\n                    # We obtain the mapping from the old to the new node IDs\n                    node_ids_mapping = train.get_node_ids_mapping_from_graph(graph)\n\n                    holdout_node_features = [\n                        holdout_node_feature[node_ids_mapping]\n                        for holdout_node_feature in holdout_node_features\n                    ]\n\n            train_of_interest = train & subgraph_of_interest\n            test_of_interest = test & subgraph_of_interest\n\n            # We validate that the two graphs are still\n            # valid for this task.\n            for graph_partition, graph_partition_name in (\n                (train_of_interest, \"train\"),\n                (test_of_interest, \"test\"),\n            ):\n                if not graph_partition.has_nodes():\n                    raise ValueError(\n                        f\"The {graph_partition_name} graph {graph_partition.get_name()} obtained from the evaluation \"\n                        f\"schema {evaluation_schema}, once filtered using the provided \"\n                        \"subgraph of interest, does not have any more nodes.\"\n                    )\n                if (\n                    cls.task_name() in (\"Edge Prediction\", \"Edge Label Prediction\")\n                    and not graph_partition.has_edges()\n                ):\n                    raise ValueError(\n                        f\"The {graph_partition_name} graph {graph_partition.get_name()} obtained from the evaluation \"\n                        f\"schema {evaluation_schema}, once filtered using the provided \"\n                        \"subgraph of interest, does not have any more edges which are \"\n                        f\"essential when running a {cls.task_name()} task.\"\n                    )\n        else:\n            train_of_interest = train\n            test_of_interest = test\n\n        additional_validation_kwargs = cls._prepare_evaluation(\n            graph=graph,\n            support=train,\n            train=train_of_interest,\n            test=test_of_interest,\n            subgraph_of_interest=subgraph_of_interest,\n            random_state=random_state * holdout_number,\n            verbose=verbose,\n            **validation_kwargs,\n        )\n\n        time_required_for_setting_up_holdout = time.time() - starting_setting_up_holdout\n\n        metadata = dict(\n            **metadata,\n            time_required_for_setting_up_holdout=time_required_for_setting_up_holdout,\n            time_required_to_compute_node_features=time_required_to_compute_node_features,\n            time_required_to_compute_node_type_features=time_required_to_compute_node_type_features,\n            time_required_to_compute_edge_type_features=time_required_to_compute_edge_type_features,\n            time_required_to_compute_edge_features=time_required_to_compute_edge_features,\n        )\n\n        holdout_performance = pd.concat(\n            [\n                classifier._train_and_evaluate_model(\n                    graph=graph,\n                    train_of_interest=train_of_interest,\n                    test_of_interest=test_of_interest,\n                    train=train,\n                    subgraph_of_interest=subgraph_of_interest,\n                    use_subgraph_as_support=use_subgraph_as_support,\n                    node_features=holdout_node_features,\n                    node_type_features=holdout_node_type_features,\n                    edge_type_features=holdout_edge_type_features,\n                    edge_features=holdout_edge_features,\n                    random_state=random_state,\n                    holdout_number=holdout_number,\n                    evaluation_schema=evaluation_schema,\n                    holdouts_kwargs=holdouts_kwargs,\n                    enable_cache=enable_cache,\n                    features_names=features_names,\n                    features_parameters=features_parameters,\n                    metadata=metadata.copy(),\n                    **additional_validation_kwargs,\n                    **validation_kwargs,\n                )\n                for classifier in cls.iterate_classifier_models(\n                    models=models, library_names=library_names, smoke_test=smoke_test\n                )\n            ]\n        )\n\n        return holdout_performance\n",
    "backend_metadata": {
        "type": "pandas",
        "columns_types": [
            "str",
            "float64",
            "float64",
            "bool",
            "float64",
            "float64",
            "float64",
            "float64",
            "float64",
            "float64",
            "float64",
            "float64",
            "float64",
            "float64",
            "float64",
            "float64",
            "float64",
            "float64",
            "float64",
            "float64",
            "float64",
            "float64",
            "float64",
            "float64",
            "float64",
            "float64",
            "float64",
            "float64",
            "float64",
            "float64",
            "str",
            "str",
            "str",
            "str",
            "int64",
            "int64",
            "str",
            "int64",
            "str",
            "bool",
            "str",
            "str",
            "str",
            "int64",
            "str",
            "str",
            "str",
            "str",
            "int64",
            "str",
            "float64",
            "float64",
            "float64",
            "float64",
            "float64",
            "float64",
            "float64",
            "float64",
            "str",
            "int64",
            "str",
            "str",
            "int64",
            "int64",
            "int64",
            "int64",
            "float64",
            "float64",
            "float64",
            "bool",
            "bool",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str"
        ],
        "columns": [
            "evaluation_mode",
            "train_size",
            "validation_unbalance_rate",
            "use_scale_free_distribution",
            "recall",
            "miss_rate",
            "prevalence_threshold",
            "fall_out",
            "false_omission_rate",
            "prevalence",
            "balanced_accuracy",
            "positive_likelyhood_ratio",
            "negative_likelyhood_ratio",
            "fowlkes_mallows_index",
            "false_discovery_rate",
            "markedness",
            "precision",
            "diagnostic_odds_ratio",
            "informedness",
            "specificity",
            "f1_score",
            "negative_predictive_value",
            "threat_score",
            "matthews_correlation_coefficient",
            "accuracy",
            "auroc",
            "auprc",
            "time_required_for_training",
            "time_required_for_evaluation",
            "time",
            "task_name",
            "model_name",
            "library_name",
            "graph_name",
            "nodes_number",
            "edges_number",
            "evaluation_schema",
            "holdout_number",
            "holdouts_kwargs",
            "use_subgraph_as_support",
            "node_feature_shapes",
            "node_type_feature_shapes",
            "edge_type_feature_shapes",
            "number_of_threads",
            "python_version",
            "platform",
            "node",
            "embiggen_version",
            "number_of_holdouts",
            "number_of_slurm_nodes",
            "time_required_to_compute_constant_node_features",
            "time_required_to_compute_constant_node_type_features",
            "time_required_to_compute_constant_edge_features",
            "time_required_for_setting_up_holdout",
            "time_required_to_compute_node_features",
            "time_required_to_compute_node_type_features",
            "time_required_to_compute_edge_type_features",
            "time_required_to_compute_edge_features",
            "features_names",
            [
                "model_parameters",
                "random_state"
            ],
            [
                "model_parameters",
                "edge_features"
            ],
            [
                "model_parameters",
                "edge_embeddings"
            ],
            [
                "model_parameters",
                "cooccurrence_iterations"
            ],
            [
                "model_parameters",
                "cooccurrence_window_size"
            ],
            [
                "model_parameters",
                "number_of_epochs"
            ],
            [
                "model_parameters",
                "number_of_edges_per_mini_batch"
            ],
            [
                "model_parameters",
                "learning_rate"
            ],
            [
                "model_parameters",
                "first_order_decay_factor"
            ],
            [
                "model_parameters",
                "second_order_decay_factor"
            ],
            [
                "model_parameters",
                "avoid_false_negatives"
            ],
            [
                "model_parameters",
                "use_scale_free_distribution"
            ],
            [
                "features_parameters",
                "random_state"
            ],
            [
                "features_parameters",
                "embedding_size"
            ],
            [
                "features_parameters",
                "epochs"
            ],
            [
                "features_parameters",
                "clipping_value"
            ],
            [
                "features_parameters",
                "number_of_negative_samples"
            ],
            [
                "features_parameters",
                "walk_length"
            ],
            [
                "features_parameters",
                "iterations"
            ],
            [
                "features_parameters",
                "window_size"
            ],
            [
                "features_parameters",
                "return_weight"
            ],
            [
                "features_parameters",
                "explore_weight"
            ],
            [
                "features_parameters",
                "max_neighbours"
            ],
            [
                "features_parameters",
                "learning_rate"
            ],
            [
                "features_parameters",
                "learning_rate_decay"
            ],
            [
                "features_parameters",
                "central_nodes_embedding_path"
            ],
            [
                "features_parameters",
                "contextual_nodes_embedding_path"
            ],
            [
                "features_parameters",
                "normalize_by_degree"
            ],
            [
                "features_parameters",
                "stochastic_downsample_by_degree"
            ],
            [
                "features_parameters",
                "normalize_learning_rate_by_degree"
            ],
            [
                "features_parameters",
                "use_scale_free_distribution"
            ],
            [
                "features_parameters",
                "dtype"
            ]
        ],
        "index_type": "int64",
        "columns_names_type": [
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "str",
            "tuple",
            "tuple",
            "tuple",
            "tuple",
            "tuple",
            "tuple",
            "tuple",
            "tuple",
            "tuple",
            "tuple",
            "tuple",
            "tuple",
            "tuple",
            "tuple",
            "tuple",
            "tuple",
            "tuple",
            "tuple",
            "tuple",
            "tuple",
            "tuple",
            "tuple",
            "tuple",
            "tuple",
            "tuple",
            "tuple",
            "tuple",
            "tuple",
            "tuple",
            "tuple",
            "tuple",
            "tuple"
        ]
    },
    "parameters": {
        "library_names": null,
        "use_subgraph_as_support": false,
        "node_type_features": [],
        "edge_type_features": null,
        "edge_features": [],
        "node_features_preprocessing_steps": null,
        "random_state": 42,
        "holdout_number": 1,
        "evaluation_schema": "Connected Monte Carlo",
        "enable_cache": true,
        "holdouts_kwargs": {
            "train_size": 0.75,
            "edge_types": [
                "minority_edge"
            ]
        },
        "subgraph_of_interest_has_compatible_nodes": true,
        "features_names": [
            "Walklets CBOW"
        ],
        "features_parameters": {
            "random_state": 42,
            "embedding_size": 100,
            "epochs": 30,
            "clipping_value": 6.0,
            "number_of_negative_samples": 10,
            "walk_length": 128,
            "iterations": 10,
            "window_size": 4,
            "return_weight": 1.0,
            "explore_weight": 1.0,
            "max_neighbours": 100,
            "learning_rate": 0.01,
            "learning_rate_decay": 0.9,
            "central_nodes_embedding_path": null,
            "contextual_nodes_embedding_path": null,
            "normalize_by_degree": false,
            "stochastic_downsample_by_degree": false,
            "normalize_learning_rate_by_degree": false,
            "use_scale_free_distribution": false,
            "dtype": "f32"
        },
        "source_node_types_names": null,
        "destination_node_types_names": null,
        "source_edge_types_names": null,
        "destination_edge_types_names": null,
        "source_nodes_prefixes": null,
        "destination_nodes_prefixes": null,
        "validation_unbalance_rates": [
            1.0
        ],
        "use_scale_free_distribution": true
    }
}